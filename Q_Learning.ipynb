{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh8w435ZnAMPrwC2motTBq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielACocolete/Q-Learning/blob/main/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1jX6phNd7Ul"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O que é Q-Learning?\n",
        "O Q-Learning é uma técnica de aprendizado por reforço que visa otimizar a função de valor (ou Q-value) com base no ambiente ou problema em questão. Aqui estão os principais pontos:\n",
        "\n",
        "Aprendizado por Reforço (RL): Imagine treinar um animal de estimação, recompensando-o a cada resposta correta. O RL segue uma lógica semelhante, onde um agente (como um programa de computador ou um robô) aprende a executar tarefas eficazmente com base em recompensas. Diferentemente da aprendizagem supervisionada, o RL não possui respostas corretas predefinidas; os agentes aprendem com a experiência e tomam decisões com base nas ações que maximizam a recompensa1.\n",
        "Q-Learning: O “Q” em Q-Learning representa a “qualidade” com que o modelo escolhe sua próxima ação, buscando melhorar essa qualidade ao longo do tempo. O algoritmo armazena valores em uma tabela chamada Tabela Q, que representa a função de valor. Simplificando, o Q-Learning busca aprender uma política que maximize a recompensa total. Ele é surpreendentemente útil para iniciar sua jornada no aprendizado por reforço1.\n",
        "Diferença entre Algoritmo e Modelo:\n",
        "Algoritmo de Aprendizado de Máquina: Um conjunto de regras e procedimentos matemáticos e estatísticos usado pelo modelo de aprendizado de máquina para identificar padrões e fazer previsões com base em dados.\n",
        "Modelo de Aprendizado de Máquina: Um programa que toma decisões ou faz previsões com base em dados, aprendendo com exemplos anteriores. No caso do Q-Learning, o algoritmo é parte do modelo, e o modelo é a tabela Q que armazena os valores de ação1.\n",
        "Q-Learning com Redes Neurais\n",
        "Aqui está a parte interessante: podemos combinar o Q-Learning com redes neurais para criar o Q-Learning Profundo (ou Deep Q-Learning). Nesse caso, utilizamos redes neurais artificiais para estimar a função Q de maneira mais eficiente. As redes neurais profundas (DQNs) integram-se ao Q-Learning, permitindo lidar com problemas mais complexos. Como funciona?"
      ],
      "metadata": {
        "id": "9Kr87W03d8d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[classic_control]  # Instala o Gym (fork do OpenAI Gym)\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Definindo a rede neural DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Memória de Replay\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return np.vstack(states), actions, rewards, np.vstack(next_states), dones\n",
        "\n",
        "# Função de ação com epsilon-greedy\n",
        "def act(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = agent(state)\n",
        "        return np.argmax(q_values.detach().numpy())\n",
        "\n",
        "# Parâmetros\n",
        "state_size = 4  # Tamanho do vetor de estado (CartPole)\n",
        "action_size = 2  # Número de ações (esquerda ou direita)\n",
        "batch_size = 32\n",
        "gamma = 0.99  # Fator de desconto\n",
        "epsilon = 1.0  # Taxa de exploração inicial\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "learning_rate = 0.001\n",
        "target_update = 10  # Frequência de atualização da rede alvo\n",
        "\n",
        "# Configuração do ambiente\n",
        "env = gym.make('CartPole-v1')\n",
        "agent = DQN(state_size, action_size)\n",
        "target_net = DQN(state_size, action_size)  # Rede alvo\n",
        "target_net.load_state_dict(agent.state_dict())\n",
        "optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "memory = ReplayBuffer(10000)\n",
        "\n",
        "# Treinamento\n",
        "episodes = 300\n",
        "for episode in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = act(state, epsilon)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        memory.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # Atualizar a rede neural\n",
        "        if len(memory.buffer) >= batch_size:\n",
        "            states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
        "            states = torch.FloatTensor(states)\n",
        "            next_states = torch.FloatTensor(next_states)\n",
        "            rewards = torch.FloatTensor(rewards)\n",
        "            dones = torch.FloatTensor(dones)\n",
        "\n",
        "            q_values = agent(states).gather(1, torch.LongTensor(actions).unsqueeze(1)).squeeze(1)\n",
        "            next_q_values = target_net(next_states).max(1)[0]\n",
        "            targets = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "            loss = (q_values - targets).pow(2).mean()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Atualizar a rede alvo\n",
        "        if episode % target_update == 0:\n",
        "            target_net.load_state_dict(agent.state_dict())\n",
        "\n",
        "    # Reduzir epsilon (exploração)\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    print(f'Episódio {episode}, Total de Recompensas: {total_reward}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xGJcb9FFeBwM",
        "outputId": "9edffe7f-b186-428c-c6be-77d33c710ec6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[classic_control]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[classic_control])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (2.6.0)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Episódio 0, Total de Recompensas: 13.0\n",
            "Episódio 1, Total de Recompensas: 27.0\n",
            "Episódio 2, Total de Recompensas: 49.0\n",
            "Episódio 3, Total de Recompensas: 15.0\n",
            "Episódio 4, Total de Recompensas: 27.0\n",
            "Episódio 5, Total de Recompensas: 15.0\n",
            "Episódio 6, Total de Recompensas: 12.0\n",
            "Episódio 7, Total de Recompensas: 19.0\n",
            "Episódio 8, Total de Recompensas: 21.0\n",
            "Episódio 9, Total de Recompensas: 12.0\n",
            "Episódio 10, Total de Recompensas: 19.0\n",
            "Episódio 11, Total de Recompensas: 20.0\n",
            "Episódio 12, Total de Recompensas: 13.0\n",
            "Episódio 13, Total de Recompensas: 44.0\n",
            "Episódio 14, Total de Recompensas: 36.0\n",
            "Episódio 15, Total de Recompensas: 12.0\n",
            "Episódio 16, Total de Recompensas: 26.0\n",
            "Episódio 17, Total de Recompensas: 23.0\n",
            "Episódio 18, Total de Recompensas: 19.0\n",
            "Episódio 19, Total de Recompensas: 11.0\n",
            "Episódio 20, Total de Recompensas: 20.0\n",
            "Episódio 21, Total de Recompensas: 44.0\n",
            "Episódio 22, Total de Recompensas: 13.0\n",
            "Episódio 23, Total de Recompensas: 18.0\n",
            "Episódio 24, Total de Recompensas: 18.0\n",
            "Episódio 25, Total de Recompensas: 21.0\n",
            "Episódio 26, Total de Recompensas: 41.0\n",
            "Episódio 27, Total de Recompensas: 19.0\n",
            "Episódio 28, Total de Recompensas: 22.0\n",
            "Episódio 29, Total de Recompensas: 27.0\n",
            "Episódio 30, Total de Recompensas: 44.0\n",
            "Episódio 31, Total de Recompensas: 11.0\n",
            "Episódio 32, Total de Recompensas: 23.0\n",
            "Episódio 33, Total de Recompensas: 16.0\n",
            "Episódio 34, Total de Recompensas: 24.0\n",
            "Episódio 35, Total de Recompensas: 25.0\n",
            "Episódio 36, Total de Recompensas: 16.0\n",
            "Episódio 37, Total de Recompensas: 29.0\n",
            "Episódio 38, Total de Recompensas: 34.0\n",
            "Episódio 39, Total de Recompensas: 59.0\n",
            "Episódio 40, Total de Recompensas: 34.0\n",
            "Episódio 41, Total de Recompensas: 25.0\n",
            "Episódio 42, Total de Recompensas: 18.0\n",
            "Episódio 43, Total de Recompensas: 53.0\n",
            "Episódio 44, Total de Recompensas: 14.0\n",
            "Episódio 45, Total de Recompensas: 27.0\n",
            "Episódio 46, Total de Recompensas: 19.0\n",
            "Episódio 47, Total de Recompensas: 23.0\n",
            "Episódio 48, Total de Recompensas: 35.0\n",
            "Episódio 49, Total de Recompensas: 78.0\n",
            "Episódio 50, Total de Recompensas: 42.0\n",
            "Episódio 51, Total de Recompensas: 23.0\n",
            "Episódio 52, Total de Recompensas: 29.0\n",
            "Episódio 53, Total de Recompensas: 28.0\n",
            "Episódio 54, Total de Recompensas: 78.0\n",
            "Episódio 55, Total de Recompensas: 17.0\n",
            "Episódio 56, Total de Recompensas: 34.0\n",
            "Episódio 57, Total de Recompensas: 41.0\n",
            "Episódio 58, Total de Recompensas: 26.0\n",
            "Episódio 59, Total de Recompensas: 24.0\n",
            "Episódio 60, Total de Recompensas: 34.0\n",
            "Episódio 61, Total de Recompensas: 46.0\n",
            "Episódio 62, Total de Recompensas: 15.0\n",
            "Episódio 63, Total de Recompensas: 16.0\n",
            "Episódio 64, Total de Recompensas: 22.0\n",
            "Episódio 65, Total de Recompensas: 105.0\n",
            "Episódio 66, Total de Recompensas: 27.0\n",
            "Episódio 67, Total de Recompensas: 26.0\n",
            "Episódio 68, Total de Recompensas: 72.0\n",
            "Episódio 69, Total de Recompensas: 34.0\n",
            "Episódio 70, Total de Recompensas: 94.0\n",
            "Episódio 71, Total de Recompensas: 43.0\n",
            "Episódio 72, Total de Recompensas: 28.0\n",
            "Episódio 73, Total de Recompensas: 33.0\n",
            "Episódio 74, Total de Recompensas: 55.0\n",
            "Episódio 75, Total de Recompensas: 22.0\n",
            "Episódio 76, Total de Recompensas: 10.0\n",
            "Episódio 77, Total de Recompensas: 38.0\n",
            "Episódio 78, Total de Recompensas: 93.0\n",
            "Episódio 79, Total de Recompensas: 11.0\n",
            "Episódio 80, Total de Recompensas: 118.0\n",
            "Episódio 81, Total de Recompensas: 37.0\n",
            "Episódio 82, Total de Recompensas: 170.0\n",
            "Episódio 83, Total de Recompensas: 48.0\n",
            "Episódio 84, Total de Recompensas: 27.0\n",
            "Episódio 85, Total de Recompensas: 63.0\n",
            "Episódio 86, Total de Recompensas: 41.0\n",
            "Episódio 87, Total de Recompensas: 85.0\n",
            "Episódio 88, Total de Recompensas: 92.0\n",
            "Episódio 89, Total de Recompensas: 86.0\n",
            "Episódio 90, Total de Recompensas: 57.0\n",
            "Episódio 91, Total de Recompensas: 19.0\n",
            "Episódio 92, Total de Recompensas: 162.0\n",
            "Episódio 93, Total de Recompensas: 52.0\n",
            "Episódio 94, Total de Recompensas: 17.0\n",
            "Episódio 95, Total de Recompensas: 63.0\n",
            "Episódio 96, Total de Recompensas: 48.0\n",
            "Episódio 97, Total de Recompensas: 149.0\n",
            "Episódio 98, Total de Recompensas: 58.0\n",
            "Episódio 99, Total de Recompensas: 125.0\n",
            "Episódio 100, Total de Recompensas: 50.0\n",
            "Episódio 101, Total de Recompensas: 57.0\n",
            "Episódio 102, Total de Recompensas: 155.0\n",
            "Episódio 103, Total de Recompensas: 140.0\n",
            "Episódio 104, Total de Recompensas: 49.0\n",
            "Episódio 105, Total de Recompensas: 182.0\n",
            "Episódio 106, Total de Recompensas: 102.0\n",
            "Episódio 107, Total de Recompensas: 75.0\n",
            "Episódio 108, Total de Recompensas: 38.0\n",
            "Episódio 109, Total de Recompensas: 110.0\n",
            "Episódio 110, Total de Recompensas: 111.0\n",
            "Episódio 111, Total de Recompensas: 48.0\n",
            "Episódio 112, Total de Recompensas: 47.0\n",
            "Episódio 113, Total de Recompensas: 48.0\n",
            "Episódio 114, Total de Recompensas: 15.0\n",
            "Episódio 115, Total de Recompensas: 187.0\n",
            "Episódio 116, Total de Recompensas: 41.0\n",
            "Episódio 117, Total de Recompensas: 94.0\n",
            "Episódio 118, Total de Recompensas: 214.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f0184fc30cea>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_q_values\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}